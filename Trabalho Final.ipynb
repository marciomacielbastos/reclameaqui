{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import Image\n",
    "import pytesseract\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "import scipy.stats as ss\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "pagina='http://www.reclameaqui.com.br/ranking/'\n",
    "driver.get(pagina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#driver = webdriver.Firefox()\n",
    "#pagina='http://www.reclameaqui.com.br/ranking/'\n",
    "#driver.get(pagina)\n",
    "#time.sleep(5)\n",
    "#driver.get(driver.find_element_by_xpath('//*[@id=\"tabela-ranking\"]/tbody/tr[14]/td/div/table[1]/tbody/tr/td/a').get_attribute(\"href\"))\n",
    "#time.sleep(5)\n",
    "#driver.get(driver.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div/ul/li/a').get_attribute(\"href\"))\n",
    "#time.sleep(5)\n",
    "#text = driver.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div[2]/ul/li[1]/span[2]').text\n",
    "#print text\n",
    "#driver.get(driver.find_element_by_xpath('//*[@id=\"todo-conteudo-resultado\"]/div/div/center/div/ul/li[7]/a').get_attribute(\"href\"))\n",
    "#time.sleep(5)\n",
    "#breakCaptha(driver)\n",
    "#time.sleep(5)\n",
    "#driver.get('http://www.google.com')\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#driver.find_element_by_class_name(\"dados-reclamacao-fanpage\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrapRanking(arg):\n",
    "    try:\n",
    "        for i in range(20):\n",
    "            arg['business'][i]=driver.find_element_by_xpath('//*[@id=\"tabela-ranking\"]/tbody/tr[14]/td/div/table['+str(i+1)+']/tbody/tr/td/a').get_attribute(\"title\")\n",
    "            arg['href'][i]=driver.find_element_by_xpath('//*[@id=\"tabela-ranking\"]/tbody/tr[14]/td/div/table['+str(i+1)+']/tbody/tr/td/a').get_attribute(\"href\")\n",
    "    except:\n",
    "        return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "ranking=pd.DataFrame(np.zeros((20,2)))\n",
    "ranking.columns=['business','href']\n",
    "scrapRanking(ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meses = [u'Janeiro', u'Fevereiro', u'Março', u'Abril', u'Maio', u'Junho', u'Julho', u'Agosto', u'Setembro', u'Outubro', u'Novembro', u'Dezembro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setDate(txt):\n",
    "    try:\n",
    "        for i in range(12):\n",
    "            txt = txt.replace(meses[i],u'0'+str(i+1))\n",
    "            txt = txt.replace(u'de', '-')\n",
    "            match = re.search(r'\\d{2} - \\d{2} - \\d{4} - \\d{2}:\\d{2}', txt)\n",
    "        return match, txt\n",
    "    except:\n",
    "        print 'Is not a String object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDate(arg):\n",
    "    match,txt = setDate(arg)\n",
    "    date = time.strptime(match.group().replace(\" \",\"\"), '%d-%m-%Y-%H:%M')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPlace(arg):\n",
    "    match, arg = setDate(arg)\n",
    "    arg = arg.replace(match.group(),\"\")\n",
    "    m = re.match(r\"(\\w+)\", arg).group(0)\n",
    "    arg = arg.replace(m,\"\")\n",
    "    match = re.search(r', , ', arg)\n",
    "    arg = arg.replace(match.group(),\"\")\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def breakCaptha(wd):\n",
    "    try:\n",
    "        oldtab = wd.current_window_handle\n",
    "        body = wd.find_element_by_tag_name(\"body\")\n",
    "        body.send_keys(Keys.CONTROL + 't')\n",
    "        wd.get('http://www.reclameaqui.com.br/indices/lista_reclamacoes/captcha.php')\n",
    "        #You have to create the folder /captcha\n",
    "        wd.save_screenshot(\"../marcim/captcha/screenshot.png\")\n",
    "        wd.find_element_by_tag_name(\"body\").send_keys(Keys.ALT + Keys.NUMPAD1)\n",
    "        wd.switch_to_window(oldtab)\n",
    "        captcha = pytesseract.image_to_string(Image.open('../marcim/captcha/screenshot.png'), config=\"-psm 6\")\n",
    "        cp = driver.find_element_by_name('captcha')\n",
    "        cp.send_keys(captcha)\n",
    "        wd.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div[2]/ul/div/div/form/input[2]').click()\n",
    "    except:\n",
    "        print \"Didn't broke the captcha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain performance, we prefer to get a list of URLs in third level of scraping, and then visit them. This already give us O(n³), if we had choose to scrap the fourth level, we would get a O(n⁴) of complexity order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def thirdLevelScrap(i):\n",
    "    try:\n",
    "        subsubtable = pd.DataFrame(np.zeros((20,4)))\n",
    "        subsubtable.columns=['business','url','date','place']\n",
    "        for j in range(20):\n",
    "            subsubtable['business'][j] = ranking['business'][i]\n",
    "            subsubtable['url'][j] = driver.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div[2]/ul/li['+str(j+1)+']/h3/a').get_attribute(\"href\")\n",
    "            text = driver.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div[2]/ul/li['+str(j+1)+']/span[2]').text\n",
    "            subsubtable['place'][j] = getPlace(text)\n",
    "            subsubtable['date'][j] = datetime.datetime(*getDate(text)[:6])\n",
    "        return subsubtable\n",
    "    except:\n",
    "            #Break the captcha and goes on\n",
    "            print 'error, possibly a captcha asking'\n",
    "            breakCaptha(driver)\n",
    "            subsubtable = thirdLevelScrap(i)\n",
    "            return subsubtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def secondLevelScrap(i):\n",
    "    try:\n",
    "        k=2015\n",
    "        subtable = pd.DataFrame(np.zeros((1,4)))\n",
    "        subtable.columns=['business','url','date','place']\n",
    "        while k>2013:\n",
    "            subsubtable = thirdLevelScrap(i);\n",
    "            k = subsubtable['date'][subsubtable['date'].size-1].year\n",
    "            #section to concatenate leafs of a business\n",
    "            if(subtable['business'].size<2):\n",
    "                subtable = subsubtable\n",
    "            pieces=[subtable,subsubtable]\n",
    "            subtable = pd.concat(pieces, ignore_index=True)\n",
    "            nextPage = driver.find_element_by_xpath('//*[@id=\"todo-conteudo-resultado\"]/div/div/center/div/ul/li[7]/a').get_attribute(\"href\")\n",
    "            driver.get(nextPage)\n",
    "            time.sleep(5)\n",
    "        return subtable\n",
    "    except:\n",
    "        print \"An exception screw everything on second level, returning whatever you got\"\n",
    "        return subtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def firstLevelScrap():\n",
    "    try:\n",
    "        table = pd.DataFrame(np.zeros((20,4)))\n",
    "        table.columns=['business','url','date','place']\n",
    "        for i in range(ranking['href'].size):\n",
    "                    driver.get(ranking['href'][i])\n",
    "                    time.sleep(5)\n",
    "                    #Take and get the url \"todas as reclamações\"\n",
    "                    key = driver.find_element_by_xpath('//*[@id=\"reclamacoes-empresa\"]/div/div[2]/div/div/ul/li/a').get_attribute(\"href\")\n",
    "                    driver.get(key)\n",
    "                    subtable = secondLevelScrap(i)\n",
    "                    #section to concatenate business\n",
    "                    pieces=[table,subtable]\n",
    "                    table=pd.concat(pieces, ignore_index=True)\n",
    "        return table\n",
    "    except:\n",
    "        print \"An exception screw everything on first level, returning whatever you got\"\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception screw everything on first level, returning whatever you got\n"
     ]
    }
   ],
   "source": [
    "k=firstLevelScrap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
